{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metad = pd.read_csv(\"data/HAM10000_metadata.csv\")\n",
    "metad\n",
    "\n",
    "L8 = pd.read_csv(\"data/hmnist_8_8_L.csv\")\n",
    "RGB8 = pd.read_csv(\"data/hmnist_8_8_RGB.csv\")\n",
    "L28 = pd.read_csv(\"data/hmnist_28_28_L.csv\")\n",
    "RGB28 = pd.read_csv(\"data/hmnist_28_28_L.csv\") \n",
    "\n",
    "\n",
    "metad.groupby(\"dx\").count()\n",
    "metad.groupby('lesion_id').count()\n",
    "dupless = metad.drop_duplicates(subset=['lesion_id'])\n",
    "\n",
    "\n",
    "metad= dupless.sort_values('lesion_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nv', 'bkl', 'mel', 'akiec', 'bcc', 'df', 'vasc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = metad.dx.unique().tolist()\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict = metad.set_index('image_id').to_dict()['dx']\n",
    "len(_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "\n",
    "def preprocess(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = \"data/HAM10000_images/\"\n",
    "ext = \".jpg\"\n",
    "\n",
    "all_images = []\n",
    "categories = []\n",
    "\n",
    "for key, value in _dict.items():\n",
    "    #load image\n",
    "    all_images.append(preprocess(_dir + key + ext))\n",
    "    categories.append(value)\n",
    "\n",
    "    #all_images.append(images)\n",
    "\n",
    "all_images\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_images)):\n",
    "    print(f\"{tags[i]} size = {len(all_images[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "lbl = LabelEncoder()\n",
    "int_enc = lbl.fit_transform(categories)\n",
    "\n",
    "print(int_enc)\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "int_enc = int_enc.reshape(len(int_enc), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(int_enc)\n",
    "print(onehot_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkl_ids = []\n",
    "nv_ids = []\n",
    "df_ids = []\n",
    "mel_ids = []\n",
    "vasc_ids = []\n",
    "bcc_ids = []\n",
    "akiec_ids = []\n",
    "\n",
    "super_list = [bkl_ids,\n",
    "              nv_ids,\n",
    "              df_ids,\n",
    "              mel_ids,\n",
    "              vasc_ids,\n",
    "              bcc_ids,\n",
    "              akiec_ids]\n",
    "\n",
    "\n",
    "for i in range(len(tags)):\n",
    "    super_list[i] = (metad[metad['dx'] == tags[i]]['image_id'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "def get_preprocessed_images(images_ids: list, image_size: tuple) -> list:\n",
    "    images = []\n",
    "    dir1 = \"data/HAM10000_images/\"\n",
    "    dir2 = \"data/HAM10000_images_part_2/\"\n",
    "    ext = \".jpg\"\n",
    "    for _id in images_ids:\n",
    "        try:\n",
    "            img = image.load_img(dir1+_id+ext, target_size=image_size)\n",
    "            img = image.img_to_array(img)\n",
    "            img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "            img = preprocess_input(img)\n",
    "            images.append(img)\n",
    "        except:\n",
    "            try:\n",
    "                #continue\n",
    "                img = image.load_img(dir2+_id+ext, target_size=image_size)\n",
    "                img = image.img_to_array(img)\n",
    "                img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "                img = preprocess_input(img)\n",
    "                images.append(img)\n",
    "            except:\n",
    "                print(dir1 + _id)\n",
    "                print(dir2 + _id)\n",
    "                print(\"WOW\")\n",
    "            \n",
    "            \n",
    "    return np.vstack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the keras preprocessing method.\n",
    "from tensorflow.keras.preprocessing import image\n",
    "image_size = (224, 224)\n",
    "# Load your images and preprocess them.\n",
    "\n",
    "bkl_images = get_preprocessed_images(super_list[0],image_size)\n",
    "print(\"Done 0\" )\n",
    "nv_images = get_preprocessed_images(super_list[1],image_size)\n",
    "print(\"Done 1\")\n",
    "df_images = get_preprocessed_images(super_list[2],image_size)\n",
    "print(\"Done 2\")\n",
    "mel_images = get_preprocessed_images(super_list[3],image_size)\n",
    "print(\"Done 3\")\n",
    "vasc_images = get_preprocessed_images(super_list[4],image_size)\n",
    "print(\"Done 4\")\n",
    "bcc_images = get_preprocessed_images(super_list[5],image_size)\n",
    "print(\"Done 5\")\n",
    "akiec_images = get_preprocessed_images(super_list[6],image_size)\n",
    "print(\"Done 6\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a numpy array for each of the class labels (one hot encoded).\n",
    "bkl_labels = np.tile([1, 0, 0, 0, 0, 0, 0], (bkl_images.shape[0], 1))\n",
    "nv_labels = np.tile([0, 1, 0, 0, 0, 0, 0], (nv_images.shape[0], 1))\n",
    "df_labels = np.tile([0, 0, 1, 0, 0, 0, 0], (df_images.shape[0], 1))\n",
    "mel_labels = np.tile([0, 0, 0, 1, 0, 0, 0], (mel_images.shape[0], 1))\n",
    "vasc_labels = np.tile([0, 0, 0, 0, 1, 0, 0], (vasc_images.shape[0], 1))\n",
    "bcc_labels = np.tile([0, 0, 0, 0, 0, 1, 0], (bcc_images.shape[0], 1))\n",
    "akiec_labels = np.tile([0, 0, 0, 0, 0, 0, 1], (akiec_images.shape[0], 1))\n",
    "\n",
    "X = np.concatenate([bkl_images,\n",
    "                    nv_images,\n",
    "                    df_images,\n",
    "                    mel_images,\n",
    "                    vasc_images,\n",
    "                    bcc_images,\n",
    "                    akiec_images])\n",
    "\n",
    "y = np.concatenate([bkl_labels,\n",
    "                    nv_labels,\n",
    "                    df_labels,\n",
    "                    mel_labels,\n",
    "                    vasc_labels,\n",
    "                    bcc_labels,\n",
    "                    akiec_labels])\n",
    "\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_X = np.vstack(all_images)\n",
    "\n",
    "print(_X[0])\n",
    "print(X[0])\n",
    "\n",
    "print(y[0])\n",
    "print(onehot_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, \n",
    "    y_train_val,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Determine the number of generated samples you want per original sample.\n",
    "datagen_batch_size = 16\n",
    "\n",
    "# Make a datagenerator object using ImageDataGenerator.\n",
    "train_datagen = ImageDataGenerator(rotation_range=60,\n",
    "                                    horizontal_flip=True)\n",
    "\n",
    "# Feed the generator your train data.\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=datagen_batch_size)\n",
    "\n",
    "# Make a datagenerator object using ImageDataGenerator.\n",
    "validation_datagen = ImageDataGenerator(rotation_range=60,\n",
    "                                        horizontal_flip=True)\n",
    "\n",
    "# Feed the generator your validation data.\n",
    "validation_generator = validation_datagen.flow(X_val, y_val, batch_size=datagen_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your chosen model!\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "# Make a model object. \n",
    "# Make sure you exclude the top part. set the input shape of the model to 224x224 pixels, with 3 color channels.\n",
    "model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "# Freeze the imported layers so they cannot be retrained.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(64, activation='relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "# Summarize.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model. Use the Adam optimizer and crossentropical loss. \n",
    "# Use the validation data argument during fitting to include your validation data.\n",
    "new_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "history = new_model.fit(train_generator,\n",
    "                        epochs=10, \n",
    "                        batch_size=500,\n",
    "                        validation_data=validation_generator\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\" This helper function takes the tensorflow.python.keras.callbacks.History\n",
    "    that is output from your `fit` method to plot the loss and accuracy of\n",
    "    the training and validation set.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "    axs[0].plot(history.history['accuracy'], label='training set')\n",
    "    axs[0].plot(history.history['val_accuracy'], label = 'validation set')\n",
    "    axs[0].set(xlabel = 'Epoch', ylabel='Accuracy', ylim=[0, .8])\n",
    "\n",
    "    axs[1].plot(history.history['loss'], label='training set')\n",
    "    axs[1].plot(history.history['val_loss'], label = 'validation set')\n",
    "    axs[1].set(xlabel = 'Epoch', ylabel='Loss', ylim=[0, 11])\n",
    "    \n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(y_val)\n",
    "#cm = confusion_matrix(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input random image\n",
    "image_id = \"ISIC_0029\"\n",
    "\"\"\"\n",
    "for i in range(len(tags)):\n",
    "        print(f\"{tags[i]},\",end=\"\")\n",
    "print(\"REAL\")\n",
    "\"\"\"\n",
    "for i in range(306,500):\n",
    "    real_id = image_id + str(i)\n",
    "    \n",
    "    filename = f\"data/HAM10000_images/{real_id}.jpg\"\n",
    "    img = image.load_img(filename, target_size=image_size)\n",
    "    print(f\"{type(img)}\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    pred = new_model.predict(img)\n",
    "    pred = pred.tolist()[0]\n",
    "    S = sum(pred)\n",
    "    pred = [round(x / S,2) for x in pred]\n",
    "    for i in range(len(tags)):\n",
    "        print(f\"{pred[i]},\",end=\"\")\n",
    "    print(metad[metad['image_id'] == real_id]['dx'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2dc215e3d66fc77cd99db91b898607bfea423b62b49cbbda505af5c230698ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
